---
title: "R Notebook"
output: 
  pdf_document: default
  html_notebook: default
  word_document: default
---

#Setting Up CRAN Repository
```{r}
#This sets the CRAN repository to ensure package installation from a reliable source.
# Set the CRAN Repository
options(repos = c(CRAN = "https://cloud.r-project.org/"))
```


#Installing Required Packages
```{r}
packages <- c("tidyverse", "openxlsx", "knitr", "dplyr", "MASS", "pscl", "pROC", "car", "tidyr", "ggplot2", "corrplot")
install_if_missing <- function(p) {
  if (!p %in% installed.packages()[, "Package"]) install.packages(p)
}
sapply(packages, install_if_missing)

```

#Setting the Working Directory and Loading Data
```{r}

setwd("G:/UMBC_Academics/HIT_750_Data Analytics/Project/Week 1")  # Path of Woring directory
list.files()

data <- read.csv("diabetes_dataset.csv")  # Dataset filename


```
#Loading the Variable Table
```{r}
# Load the openxlsx package to work with Excel files
library(openxlsx)

# Load the knitr package for rendering tables
library(knitr)

# Set the path to the Excel file on your local machine
# Update this path to where the file is located on your system
file_path <- "G:/UMBC_Academics/HIT_750_Data Analytics/Project/Week 1/VariableTable.xlsx"

# Read the Excel sheet into a variable (assuming it's in the first sheet)
variable <- read.xlsx(file_path, sheet = 1)

# View the data in the notebook using knitr's kable function
kable(variable, caption = "Variable Table from Excel")

```


#Calculating Summary Statistics for Numeric Variables
```{r}
# Load necessary packages
library(knitr)
library(dplyr)
library(tidyr)

# Set the correct path to your local dataset
data <- read.csv("G:/UMBC_Academics/HIT_750_Data Analytics/Project/Week 1/diabetes_dataset.csv")

# Function to calculate summary statistics
calc_summary_stats <- function(x) {
  c(
    Min = min(x, na.rm = TRUE),
    `1st Qu.` = quantile(x, 0.25, na.rm = TRUE),
    Median = median(x, na.rm = TRUE),
    Mean = mean(x, na.rm = TRUE),
    `3rd Qu.` = quantile(x, 0.75, na.rm = TRUE),
    Max = max(x, na.rm = TRUE)
  )
}

# Apply the function to each numeric variable in the dataset
summary_stats <- data %>%
  select_if(is.numeric) %>%
  summarise_all(list(calc_summary_stats))

# Unnest the list columns generated by summarise_all
summary_stats_unnested <- summary_stats %>%
  unnest(cols = everything(), names_repair = "minimal")

# Transpose the summary statistics for better readability
summary_stats_t <- t(summary_stats_unnested)

# Convert the transposed summary stats into a data frame
summary_df <- as.data.frame(summary_stats_t)

# Set proper column names after transposing
colnames(summary_df) <- c("Min", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max")

# Add the variable names as a separate column
summary_df$Variable <- rownames(summary_df)

# Reorder the columns to place the Variable column first
summary_df <- summary_df[, c("Variable", "Min", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max")]

# Print the final summary_df to debug
print(summary_df)

# Print the formatted summary statistics table
kable(summary_df, format = "latex", row.names = FALSE)

```

#Displaying Dataset Summary and Structure
```{r}
# Summary statistics for the dataset
summary(data)

# Check the structure of the dataset
str(data)

```

#Check for missing values in the dataset
```{r}
# Check for missing values in the dataset
missing_values <- colSums(is.na(data))

# Filter out variables with missing values and create a neat output
missing_summary <- missing_values[missing_values > 0]

# Print the number of missing values in a neat format
if(length(missing_summary) > 0) {
  cat("Number of missing values in the dataset:\n")
  for(variable in names(missing_summary)) {
    cat(variable, ":", missing_summary[variable], "\n")
  }
} else {
  cat("There are no missing values in the dataset.\n")
}

```

```{r}
install.packages("ggplot2")

```
#Plotting the Distribution of BMI
```{r}
library(ggplot2)
# Histogram of BMI
ggplot(data, aes(x = BMI)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "black") +
  labs(title = "Distribution of BMI", x = "BMI", y = "Frequency")

```

#Counting Individuals with High Blood Pressure
```{r}
# Count the number of people with and without high blood pressure
highbp_count <- table(data$HighBP)

# Display the counts in a neat format
highbp_count_df <- as.data.frame(highbp_count)
colnames(highbp_count_df) <- c("HighBP_Status", "Count")

# Print the summary
print(highbp_count_df)

```

#Plotting High Cholesterol Status
```{r}
# Load ggplot2 if not already loaded
library(ggplot2)

# Create a bar plot for High Cholesterol status
ggplot(data, aes(x = as.factor(HighChol))) +
  geom_bar(fill = "lightcoral") +
  labs(title = "Count of Individuals with and without High Cholesterol",
       x = "High Cholesterol (0 = No, 1 = Yes)",
       y = "Count") +
  scale_x_discrete(labels = c("0" = "No", "1" = "Yes")) +
  theme_minimal()

```


#Correlation Analysis and Plotting
```{r}
install.packages("corrplot")
# Load necessary package
library(corrplot)

# Calculate the correlation matrix
cor_matrix <- cor(select(data, where(is.numeric)))

# Create a correlation plot
corrplot(cor_matrix, method = "circle")
```


#BMI by High Blood Pressure
```{r}
# Boxplot of BMI by High Blood Pressure
ggplot(data, aes(x = as.factor(HighBP), y = BMI)) +
  geom_boxplot(fill = "skyblue") +
  labs(title = "BMI Distribution by High Blood Pressure Status", x = "High Blood Pressure (0 = No, 1 = Yes)", y = "BMI") +
  theme_minimal()

```


```{r}
getwd()
```

# Step 3: Inferential Statistics - Chi-Square Test for Categorical Variables
The Chi-Square test is used to evaluate the association between categorical variables. In this project, we are interested in understanding how certain factors, such as high blood pressure, high cholesterol, and physical activity, relate to diabetes status. Specifically, we aim to determine whether the distribution of diabetes status (diabetic vs. non-diabetic) is independent of these categorical factors.

This section evaluates the association between categorical variables (`HighBP`, `HighChol`, `PhysActivity`) and diabetes status.
```{r}
# Disable scientific notation
options(scipen=999)

# Chi-Square test for High Blood Pressure and Diabetes Status
chisq_highbp <- chisq.test(data$HighBP, data$Diabetes_binary)
print(chisq_highbp)  # Print the results

# Chi-Square test for High Cholesterol and Diabetes Status
chisq_highchol <- chisq.test(data$HighChol, data$Diabetes_binary)
print(chisq_highchol)  # Print the results

# Chi-Square test for Physical Activity and Diabetes Status
chisq_physactivity <- chisq.test(data$PhysActivity, data$Diabetes_binary)
print(chisq_physactivity)  # Print the results

# Re-enable scientific notation 
options(scipen=0)


```
## Chi-Squared Value (X-squared):

This value indicates the strength of the association between the categorical variables. A higher chi-squared value suggests a greater discrepancy between the observed frequencies and the expected frequencies under the null hypothesis (which states that there is no association between the variables).

## Degrees of Freedom (df):

This value is determined by the number of categories in each variable. In this case, since both variables in each test are binary (e.g., HighBP is Yes/No), the degrees of freedom is 1 (calculated as (number of categories in variable A - 1) * (number of categories in variable B - 1)).

## p-value:

The p-value indicates the probability of observing the data (or something more extreme) if the null hypothesis were true. A very small p-value (typically < 0.05) suggests that you can reject the null hypothesis.
In our output, the p-values for all three tests are extremely small (< 0.00000000000000022), indicating that there is a statistically significant association between the categorical variables and diabetes status.

## Step 4: Inferential Statistics - T-test for Continuous Variable

In this step, we will conduct a Welch Two Sample t-test to compare the means of Body Mass Index (BMI) between individuals with diabetes and those without.


# T-test for BMI

The t-test is a statistical method used to determine whether there is a significant difference between the means of two groups. We will perform the t-test using the following code:

```{r}
# Perform Welch Two Sample t-test
t_test_result <- t.test(data$BMI ~ data$Diabetes_binary)

# Extract components from the test result
t_statistic <- t_test_result$statistic
df <- t_test_result$parameter
p_value <- t_test_result$p.value
conf_int <- t_test_result$conf.int
mean_group1 <- t_test_result$estimate[1]
mean_group2 <- t_test_result$estimate[2]

# Formatting the output
cat("Welch Two Sample t-test\n\n")
cat("Data: `data$BMI` by `data$Diabetes_binary`\n\n")
cat(paste("t-statistic: ", round(t_statistic, 2), "\n", sep = ""))
cat(paste("Degrees of freedom (df): ", round(df, 3), "\n", sep = ""))
cat(paste("p-value: ", format.pval(p_value, digits = 20), "\n\n", sep = ""))
cat("Alternative Hypothesis:\n")
cat("The true difference in means between the group \"No Diabetes\" and the group \"Diabetes or Pre-diabetes\" is not equal to 0.\n\n")
cat("95% Confidence Interval:\n")
cat(paste("• Lower bound: ", round(conf_int[1], 6), "\n", sep = ""))
cat(paste("• Upper bound: ", round(conf_int[2], 6), "\n\n", sep = ""))
cat("Sample Estimates:\n")
cat(paste("• Mean BMI in the \"No Diabetes\" group: ", round(mean_group1, 5), "\n", sep = ""))
cat(paste("• Mean BMI in the \"Diabetes or Pre-diabetes\" group: ", round(mean_group2, 5), "\n", sep = ""))


```
## t-statistic: -99.92

This value measures the size of the difference relative to the variation in the sample data. A large absolute value (far from zero) suggests that the means of the two groups are significantly different. 

## 95% Confidence Interval:

Lower bound: -4.219416
Upper bound: -4.057065
This interval estimates the range in which the true difference in means lies with 95% confidence. Since both bounds are negative, it reinforces the conclusion that the mean BMI in the "Diabetes or Pre-diabetes" group is significantly higher than in the "No Diabetes" group.

## Alternative Hypothesis:

The true difference in means between the group 'No Diabetes' and the group 'Diabetes or Pre-diabetes' is not equal to 0. This indicates that the analysis was set up to test whether the means of the two groups are significantly different from each other.

#Random Forest Model for Classification

This analysis implements a Random Forest classification model to predict a binary outcome (Diabetes_binary) using health-related predictors (BMI, HighBP, HighChol, PhysActivity). The model is evaluated for performance and variable importance is analyzed.

```{r}
# Install and load required libraries
if (!require("randomForest")) install.packages("randomForest", dependencies = TRUE)
if (!require("caret")) install.packages("caret", dependencies = TRUE)
if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE)

library(randomForest)
library(caret)
library(dplyr)


# Step 2: Data preprocessing
# Ensure 'Diabetes_binary' is a factor for classification
data$Diabetes_binary <- as.factor(data$Diabetes_binary)

# Check for missing values
sum(is.na(data))  # If there are missing values, consider imputation

# Step 3: Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(data$Diabetes_binary, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Step 4: Train the Random Forest model
set.seed(123)  # For reproducibility
rf_model <- randomForest(
  Diabetes_binary ~ BMI + HighBP + HighChol + PhysActivity,
  data = train_data,
  ntree = 500,  # Number of trees
  mtry = 3,     # Number of predictors sampled for splitting at each node
  importance = TRUE  # To compute variable importance
)

# Print model summary
print(rf_model)

# Step 5: Evaluate model performance on the test set
test_predictions <- predict(rf_model, newdata = test_data)

# Confusion matrix
confusionMatrix(test_predictions, test_data$Diabetes_binary)

# Step 6: Feature importance
importance_values <- importance(rf_model)
varImpPlot(rf_model)

# Step 7: Optional - Save the model for future use
saveRDS(rf_model, "random_forest_model.rds")

# Step 8: Load the saved model (if needed later)
# rf_model <- readRDS("random_forest_model.rds")

```

#1. Random Forest Model Summary

Type of Random Forest: Classification model.
Number of Trees: 500 decision trees were built.
Variables Tried at Each Split: 3 predictors were randomly sampled at each split.
Out-of-Bag (OOB) Error Rate: 13.79%. This indicates the model's average classification error rate using the OOB samples.

##2. Confusion Matrix (OOB Data)
Class Error:
Class 0 has an error rate of 0.59% (high accuracy for classifying negatives).
Class 1 has an error rate of 95.33% (low accuracy for classifying positives).
The model struggles to classify the minority class (1), leading to high class error.

##3. Confusion Matrix and Statistics (Test Data)
Accuracy: 86.21%, meaning the model correctly classifies ~86% of observations overall.
Kappa: 0.063, indicating weak agreement between predicted and actual classes beyond chance.
Sensitivity (Recall for 0): 99.46%, meaning the model detects almost all negative cases (0).
Specificity (Recall for 1): 4.40%, meaning the model detects very few positive cases (1).
Positive Predictive Value (PPV) for 0: 86.54%, meaning most predictions of 0 are correct.
Negative Predictive Value (NPV) for 1: 56.81%, indicating relatively poor predictive ability for 1.
Balanced Accuracy:
The balanced accuracy is 51.93%, close to random guessing (50%). This is due to imbalanced class distribution.

# Why Resampling is Necessary
1. Class Imbalance Issue
In the dataset, class 0 is significantly more frequent (majority class) than class 1 (minority class).
The model prioritizes the majority class, leading to:
High sensitivity and low specificity.
Poor detection of the minority class (1), as shown by the confusion matrix and class error.
2. Impact of Imbalanced Data
Accuracy alone is misleading because the model achieves high overall accuracy by focusing on the majority class (0).
Metrics like Kappa, specificity, and balanced accuracy reveal the poor performance on minority class detection.
3. Need for Resampling Techniques

To address the imbalance and improve the model’s performance for both classes:

Oversampling: Increase the frequency of the minority class (1) by creating synthetic samples (e.g., SMOTE).
Undersampling: Reduce the majority class size to match the minority class.
Hybrid Techniques: Combine oversampling and undersampling for better balance.
Class Weights: Penalize misclassifications of the minority class more heavily to make the model focus on it.

## Expected Benefits
Improved specificity (better detection of class 1).
Balanced accuracy closer to 100%, indicating improved performance across both classes.
More robust predictions for real-world scenarios with imbalanced data

```{r}
writeLines('PATH="${RTOOLS44_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")

```
# Resampling

ROSE (Random Over-Sampling Examples) is an R package designed to handle class imbalance in datasets. It generates synthetic data by creating new samples for the minority class and/or reducing the samples of the majority class. The goal is to create a more balanced dataset for training machine learning models.

```{r}
 # Install and load the ROSE package
if (!require("ROSE")) install.packages("ROSE", dependencies = TRUE)
library(ROSE)

# Apply ROSE to balance the training data
set.seed(123)  # For reproducibility
balanced_train_data <- ROSE(
  Diabetes_binary ~ ., 
  data = train_data, 
  seed = 123
)$data

# Check the class distribution after balancing
cat("Class distribution in the balanced training data:\n")
print(table(balanced_train_data$Diabetes_binary))

```

```{r}
# Train Random Forest on the ROSE-balanced dataset
rf_model <- randomForest(Diabetes_binary ~ BMI + HighBP + HighChol + PhysActivity,
                         data = balanced_train_data, 
                         ntree = 500, 
                         mtry = 3, 
                         importance = TRUE)

# Print model summary
print(rf_model)

# Predict on the test dataset
predictions <- predict(rf_model, test_data)

# Confusion matrix
conf_matrix <- confusionMatrix(predictions, test_data$Diabetes_binary)

# Print confusion matrix and metrics
print(conf_matrix)

```
# Random forest with Resampled dataset

Out-Of-Bag (OOB) Error Rate: 32.01%

This is the error rate calculated on the training dataset using out-of-bag samples (data not used to build a specific tree). It indicates the model's ability to generalize.
Observation: The OOB error is relatively high, suggesting room for improvement.
Confusion Matrix (Training Set):

Class 0:
Correctly Predicted: 58,958
Incorrectly Predicted: 29,983 (misclassified as Class 1)
Class Error: 33.71%
Class 1:
Correctly Predicted: 61,768
Incorrectly Predicted: 26,868 (misclassified as Class 0)
Class Error: 30.31%
Evaluation Metrics on Test Data
Accuracy: 69.13%

Overall percentage of correctly predicted instances.
Improved compared to earlier models where accuracy was much lower due to imbalance.
Sensitivity (Recall for Class 0): 69.16%

Indicates how well the model detects instances of Class 0 (true positives).
Specificity (Recall for Class 1): 68.92%

Indicates how well the model detects instances of Class 1 (true negatives).
Kappa Statistic: 0.2283

Measures the agreement between predicted and actual values beyond chance.
Low value suggests moderate improvement but still room for optimization.
Balanced Accuracy: 69.04%

Average of sensitivity and specificity, providing a better measure for imbalanced datasets.
Improvement: Earlier results showed significantly lower balanced accuracy due to the imbalance.
McNemar's Test P-Value: <2e-16

Indicates that the difference in prediction errors for the two classes is statistically significant.
Improvements Compared to the Previous Model
Class Balancing: The use of ROSE resulted in a more balanced dataset, improving the model’s ability to generalize across both classes.

Accuracy: Improved from the previous model, which heavily favored the majority class.
Balanced Performance: Earlier models had very high sensitivity but extremely poor specificity. With ROSE, the performance is more balanced, with both sensitivity and specificity improving.
Minority Class (Class 1) Detection: The model now recognizes more Class 1 instances compared to previous models where they were severely under-predicted.

# XGBoost 

XGBoost (Extreme Gradient Boosting) is a highly efficient and flexible algorithm that excels in many machine learning tasks, including binary classification. When dealing with imbalanced datasets, applying class weights (via the scale_pos_weight parameter) can significantly improve the model's ability to handle class imbalance.


```{r}
install.packages("xgboost")

# Load necessary libraries
library(xgboost)
library(caret)

# Check and encode the target variable
train_data$Diabetes_binary <- as.numeric(train_data$Diabetes_binary)  # Ensure numeric
train_data$Diabetes_binary <- ifelse(train_data$Diabetes_binary == 1, 1, 0)  # Encode as 0/1

test_data$Diabetes_binary <- as.numeric(test_data$Diabetes_binary)
test_data$Diabetes_binary <- ifelse(test_data$Diabetes_binary == 1, 1, 0)  # Encode as 0/1

# Prepare the training and testing data
train_matrix <- as.matrix(train_data[, -which(names(train_data) == "Diabetes_binary")])
train_labels <- as.numeric(train_data$Diabetes_binary)

test_matrix <- as.matrix(test_data[, -which(names(test_data) == "Diabetes_binary")])
test_labels <- as.numeric(test_data$Diabetes_binary)

# Create DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)

# Calculate scale_pos_weight for imbalanced data
pos_weight <- sum(train_labels == 0) / sum(train_labels == 1)

# Set parameters for the XGBoost model
params <- list(
  objective = "binary:logistic",   # Binary classification
  eval_metric = "logloss",        # Log loss metric
  scale_pos_weight = pos_weight,  # Balance class weights
  eta = 0.1,                      # Learning rate
  max_depth = 6,                  # Depth of trees
  subsample = 0.8,                # Subsample ratio
  colsample_bytree = 0.8          # Column subsample ratio
)

# Train the XGBoost model
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,                       # Number of boosting rounds
  watchlist = list(train = dtrain),    # Monitor training performance
  print_every_n = 10                   # Print progress every 10 rounds
)

# Predict on the test dataset
predictions <- predict(xgb_model, dtest)

# Convert probabilities to binary labels
predicted_labels <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model using confusion matrix
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(test_labels))

# Print evaluation metrics
print(conf_matrix)

# Feature importance
importance <- xgb.importance(feature_names = colnames(train_matrix), model = xgb_model)
print(importance)

# Plot feature importance
xgb.plot.importance(importance)


```

# XGBoost summary 


Accuracy:
72.05%: Indicates that 72% of the predictions (both classes) are correct. While accuracy is high, it can be misleading in imbalanced datasets, as it often reflects the majority class.

Sensitivity (Recall for Positive Class):
80.52%: Measures the model's ability to correctly predict the majority class (class 0).
A higher sensitivity compared to previous results demonstrates improved detection of the majority class.

Specificity (Recall for Minority Class):
70.68%: Measures the model's ability to correctly identify the minority class (class 1).
This is much better than earlier models, such as the unbalanced Random Forest, which showed near-zero specificity, meaning it rarely detected the minority class correctly.

Balanced Accuracy:
75.60%: The average of sensitivity and specificity. It accounts for imbalance better than raw accuracy and shows the model balances both classes reasonably well.

Positive Predictive Value (Precision for Class 0):
30.77%: Measures how many of the predicted positives (class 0) are true positives. The lower precision reflects that there are still many false positives, but this is expected in imbalanced datasets.

Negative Predictive Value (Precision for Class 1):
95.73%: Indicates that most of the predicted negatives (class 1) are accurate. This value is very high, meaning false negatives are rare.

Kappa:
0.3052: A measure of how well the model performs compared to random chance. The moderate Kappa score indicates some improvement in prediction reliability over previous results.

Log Loss (Training Progress):
The gradual decrease in log loss during training rounds shows that the model is converging and learning effectively. The final value of 0.499491 reflects a well-trained model.


# XGBoost with Resampled dataset

```{r}
# Install and load required packages
if (!require("ROSE")) install.packages("ROSE", dependencies = TRUE)
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
library(ROSE)
library(xgboost)
library(caret)

# Apply ROSE to balance the training data
set.seed(123)  # For reproducibility
balanced_train_data <- ROSE(
  Diabetes_binary ~ ., 
  data = train_data, 
  seed = 123
)$data

# Check the class distribution after balancing
cat("Class distribution in the balanced training data:\n")
print(table(balanced_train_data$Diabetes_binary))

# Encode the target variable for XGBoost
balanced_train_data$Diabetes_binary <- as.numeric(balanced_train_data$Diabetes_binary)  # Ensure numeric
test_data$Diabetes_binary <- as.numeric(test_data$Diabetes_binary)

# Prepare the training and testing data
train_matrix <- as.matrix(balanced_train_data[, -which(names(balanced_train_data) == "Diabetes_binary")])
train_labels <- as.numeric(balanced_train_data$Diabetes_binary)

test_matrix <- as.matrix(test_data[, -which(names(test_data) == "Diabetes_binary")])
test_labels <- as.numeric(test_data$Diabetes_binary)

# Create DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)

# Set parameters for the XGBoost model
params <- list(
  objective = "binary:logistic",   # Binary classification
  eval_metric = "logloss",        # Log loss metric
  eta = 0.1,                      # Learning rate
  max_depth = 6,                  # Depth of trees
  subsample = 0.8,                # Subsample ratio
  colsample_bytree = 0.8          # Column subsample ratio
)

# Train the XGBoost model
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,                       # Number of boosting rounds
  watchlist = list(train = dtrain),    # Monitor training performance
  print_every_n = 10                   # Print progress every 10 rounds
)

# Predict on the test dataset
predictions <- predict(xgb_model, dtest)

# Convert probabilities to binary labels
predicted_labels <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model using confusion matrix
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(test_labels))

# Print evaluation metrics
print(conf_matrix)

# Feature importance
importance <- xgb.importance(feature_names = colnames(train_matrix), model = xgb_model)
print(importance)

# Plot feature importance
xgb.plot.importance(importance)

```


## Results:

1. Class Distribution in Balanced Training Data:
After applying ROSE, the training data is balanced with almost equal numbers of 0 (non-diabetic) and 1 (diabetic) cases:

Class 0: 88,636
Class 1: 88,941

This addresses the class imbalance issue and helps the model learn equally from both classes.

2. Training Log-Loss:
Log-loss measures how well the predicted probabilities align with the true labels. A lower log-loss value indicates better 
model performance:
At 100 rounds, the final log-loss is 0.307, showing a significant improvement in model fit during training.

3. Confusion Matrix:

Predicted 0:
6,187 cases correctly identified as 0 (true negatives).
10,319 cases wrongly identified as 0 (false negatives).

Predicted 1:
44,416 cases wrongly identified as 1 (false positives).
55,181 cases correctly identified as 1 (true positives).

4. Evaluation Metrics:

Accuracy:
80.64% of predictions are correct.
Indicates overall performance but is not sufficient for imbalanced data.

Sensitivity (Recall for Class 0):
58.35%: The model correctly identifies 58.35% of non-diabetic cases.
Slightly lower than earlier models because ROSE focuses on balancing both classes, which can reduce performance for the dominant class.

Specificity (Recall for Class 1):
84.25%: The model correctly identifies 84.25% of diabetic cases.
Improved significantly compared to earlier models, which struggled with minority class performance.

Balanced Accuracy:
71.30%: The average of sensitivity and specificity.
Better reflects performance on imbalanced data compared to raw accuracy.

Kappa:
0.3454: Indicates moderate agreement between predictions and true labels, showing improvement from earlier models.

5. McNemar's Test:
P-value < 2e-16: Indicates a significant difference between the types of errors (false positives vs. false negatives).

###How the Results Improved From Previous Models:

1. Addressing Imbalance:
Earlier models struggled with class imbalance, leading to poor recall for the minority class (1).
After using ROSE, the balanced training data helped the model better recognize the minority class.

2. Specificity Improvement:
Specificity increased significantly (84.25%) compared to prior models, meaning the model is much better at identifying diabetic cases (class 1).

3. Balanced Performance:
The balanced accuracy improved (71.30%), showing that the model is performing more equitably across both classes.

4. Predictive Value:
The negative predictive value (92.59%) is strong, meaning the model is highly reliable in predicting 0 (non-diabetic) cases.
Trade-Offs


